{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "source": [
    "!pip3 install transformers"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: transformers in /usr/local/lib/python3.9/site-packages (4.29.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.9/site-packages (from transformers) (0.14.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/site-packages (from transformers) (2023.5.5)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/site-packages (from requests->transformers) (2022.9.24)\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: You are using pip version 22.0.4; however, version 23.2.1 is available.\n",
      "You should consider upgrading via the '/usr/local/opt/python@3.9/bin/python3.9 -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "import torch\n",
    "\n",
    "# panda view settings\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.max_rows', None)  # Display all rows\n",
    "pd.set_option('display.max_colwidth', 500)  # Display full text in columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "source": [
    "DEVICE = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(DEVICE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "source": [
    "df = pd.read_csv(\"./data/toxicbias_train.csv\")\n",
    "df_bias = df[df['bias'] == 'bias']\n",
    "df_neutral = df[df['bias'] == 'neutral']\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split up dataframe by category"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "source": [
    "# store unique categories\n",
    "unique_categories = set()\n",
    "\n",
    "# Iterate through each entry in the 'category' column\n",
    "for categories in df['category'].str.split(','):\n",
    "    for category in categories:\n",
    "        stripped_category = category.strip()\n",
    "        if stripped_category and stripped_category.lower() != 'none':\n",
    "            unique_categories.add(stripped_category)\n",
    "\n",
    "unique_categories_list = sorted(list(unique_categories))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "source": [
    "categorical_dfs = {category: pd.DataFrame(columns=df.columns) for category in unique_categories_list}\n",
    "\n",
    "# Split categories and add rows to the corresponding dataframes in dictionary\n",
    "def split_categories_and_add_rows(row):\n",
    "    categories = row['category'].split(',')\n",
    "    for category in categories:\n",
    "        category = category.strip()  # Remove leading/trailing spaces\n",
    "        if category in categorical_dfs:\n",
    "            categorical_dfs[category] = categorical_dfs[category].append(row, ignore_index=True)\n",
    "\n",
    "df_bias.apply(split_categories_and_add_rows, axis=1)\n",
    "\n",
    "# Print the shape of each category dataframe\n",
    "for category, category_df in categorical_dfs.items():\n",
    "    print(f\"Dataframe for {category}:\")\n",
    "    print(f\"Shape: {category_df.shape}\")\n",
    "    print(\"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe for gender:\n",
      "Shape: (293, 6)\n",
      "\n",
      "\n",
      "Dataframe for lgbtq:\n",
      "Shape: (453, 6)\n",
      "\n",
      "\n",
      "Dataframe for political:\n",
      "Shape: (172, 6)\n",
      "\n",
      "\n",
      "Dataframe for race:\n",
      "Shape: (1768, 6)\n",
      "\n",
      "\n",
      "Dataframe for religion:\n",
      "Shape: (1257, 6)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split training and test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "source": [
    "training_data = {}\n",
    "test_data = {}\n",
    "\n",
    "# Perform the 80-20 split for each category dataframe\n",
    "for category, df in categorical_dfs.items():\n",
    "    train_df, test_df = train_test_split(df, test_size=0.8, random_state=42)\n",
    "    \n",
    "    training_data[category] = train_df\n",
    "    test_data[category] = test_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "source": [
    "for category, train_df in training_data.items():\n",
    "    print('Training data')\n",
    "    print(f\"Dataframe for {category}:\")\n",
    "    print(f\"Shape: {train_df.shape}\")\n",
    "    print(\"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data\n",
      "Dataframe for gender:\n",
      "Shape: (58, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for lgbtq:\n",
      "Shape: (90, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for political:\n",
      "Shape: (34, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for race:\n",
      "Shape: (353, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for religion:\n",
      "Shape: (251, 6)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "source": [
    "for category, test_df in test_data.items():\n",
    "    print('Test data')\n",
    "    print(f\"Dataframe for {category}:\")\n",
    "    print(f\"Shape: {test_df.shape}\")\n",
    "    print(\"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data\n",
      "Dataframe for gender:\n",
      "Shape: (470, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for lgbtq:\n",
      "Shape: (726, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for political:\n",
      "Shape: (276, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for race:\n",
      "Shape: (2282, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for religion:\n",
      "Shape: (1873, 6)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Premises"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "source": [
    "hypotheses = {\n",
    "  'gender' : [\n",
    "    'A person belonging to a gender lacks intelligence.',\n",
    "    'A person belonging to a gender is a criminal.',\n",
    "    'A person belonging to a gender is violent.',\n",
    "    'Violence is encouraged against a gender.',\n",
    "    'A person belonging to a gender is abusive.',\n",
    "    'A person belonging to a gender should be blamed.',\n",
    "    'Women make decisions based on emotions.',\n",
    "    'A gender is unable to do some jobs.',\n",
    "    'A gender is unable to do something as well as another gender.',\n",
    "    'A gender should not talk.',\n",
    "    'A person belonging to a gender is compared to an object.',\n",
    "    'A gender should be blamed.',\n",
    "    'A person belonging to a gender has an advantage compared to others.',\n",
    "    'A person belonging to a gender is misogynist.',\n",
    "    'A person belonging to a gender is disliked.',\n",
    "  ],\n",
    "  'lgbtq' : [\n",
    "    'Lgbtq persons are believed to have a condition.',\n",
    "    'Lgbtq should not be part of society.',\n",
    "    'Violence is encouraged against lgbtq persons.',\n",
    "    'Lgbtq persons are compared to animals.',\n",
    "    'Lgbtq persons are believed to have an agenda.',\n",
    "    'Lgbtq persons should be excluded.',\n",
    "    'Lgbtq persons should stay home.',\n",
    "    'Lgbtq persons commit crimes.',\n",
    "    'Lgbtq persons get preferred to other persons.',\n",
    "    'Lgbtq persons are seen as not being normal.', \n",
    "    'Lgbtq persons should not be allowed to marry.', \n",
    "    'Lgbtq persons have a disease.',\n",
    "    'Lgbtq persons are seen as being disgusting.',\n",
    "    'Lgbtq persons are not tolerated or desired.',\n",
    "    'Lgbtq persons are treated differently.'\n",
    "\n",
    "  ],\n",
    "  'political' : [ \n",
    "      'A person belonging to a political group is stupid.',\n",
    "      'A person belonging to a political group is crazy.',\n",
    "      'Things are worse due to a political group.',\n",
    "      'Members of a political group are lgbtq.',\n",
    "      'Members of a political group are hypocritical.',\n",
    "      'Members of a political group dislike a gender or lgbtq.',\n",
    "      'Members of a political group commit crimes.',\n",
    "      'Members of a political group are violent.',\n",
    "      'Members of a political group are hateful.',\n",
    "      'Members of a political group are compared to Nazis.',\n",
    "      'Members of a political group are racist.',\n",
    "      'A person belonging to a political group has a disease.',\n",
    "      'Members of a political group are taking control.',\n",
    "      'Members of a political group prefer minorities.',\n",
    "      'Members of a political group hate their country or race.',\n",
    "  ],\n",
    "  'race' : [\n",
    "    'Members of a race commit crimes.',\n",
    "    'Members of a race are violent.',\n",
    "    'Members of a race are compared to animals.',\n",
    "    'The use of violence is encouraged or desired against a race.',\n",
    "    'Members of a race should remain in their respective countries.',\n",
    "    'The integration of a race should be avoided in a country or society.',\n",
    "    'Members of a race are a threat to others.',\n",
    "    'A race is inferior when compared to another race.',\n",
    "    'A race is preferred to another.',\n",
    "    'Members of a race are stupid.',\n",
    "    'There are too many members of a race.',\n",
    "    'Members of a race should be excluded.',\n",
    "    'Members of a race should go back to their homes.',\n",
    "    'Members of a race are refused.',\n",
    "    'Members of a race are racist.'\n",
    "  ],\n",
    "  'religion' : [\n",
    "    'The use of violence is encouraged or desired against a religion.',\n",
    "    'A religion should be discriminated against.',\n",
    "    'Members of a religion should be excluded and marginalized.',\n",
    "    'Members of a religion should remain in their respective countries.',\n",
    "    'Members of a religion are compared to animals.',\n",
    "    'A religion or members of a religion are compared to devils or demons.',\n",
    "    'The integration of a religion should be avoided in a country or society.',\n",
    "    'A religion or members of a religion are stupid.',\n",
    "    'A religion is inferior when compared to another religion.',\n",
    "    'A religion is preferred to another.'\n",
    "    'Religious practices of a particular faith should be restricted or banned in public spaces.',\n",
    "    'Religious attire or symbols should be prohibited.',\n",
    "    'A religion promotes violence or intolerance.',\n",
    "    'Members of a religion are a threat.',\n",
    "    'Members of a religion are not trustworthy or loyal.'\n",
    "  ],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add neutral cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "source": [
    "def add_rows_to_dataframes(dataframes_dict, new_dataframe):\n",
    "\n",
    "    for key, df in dataframes_dict.items():\n",
    "        # Calculate the maximum number of rows to add\n",
    "        max_rows_to_add = min(len(new_dataframe), len(df))\n",
    "\n",
    "        # Choose random rows from the new dataframe\n",
    "        rows_to_add = new_dataframe.sample(n=max_rows_to_add, replace=True)\n",
    "\n",
    "        rows_to_add = new_dataframe.iloc[:max_rows_to_add]\n",
    "        df = pd.concat([df, rows_to_add], ignore_index=True)\n",
    "        dataframes_dict[key] = df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "source": [
    "add_rows_to_dataframes(test_data, df_neutral)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bart model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "source": [
    "def convert_probabilities_batched(probabilities, label_mapping):\n",
    "    probabilities_list = probabilities.tolist()  # Convert the tensor to a list\n",
    "    return [{name: round(pred * 100, 1) for pred, name in zip(preds, label_mapping)} for preds in probabilities_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "def bart_nli_batched_hypotheses(premises, hypotheses):\n",
    "    model_name = 'facebook/bart-large-mnli'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model.to(DEVICE) \n",
    "\n",
    "    num_premises = len(premises)\n",
    "    num_hypotheses = len(hypotheses)\n",
    "    results = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_premises):\n",
    "            labels = []\n",
    "\n",
    "            # Tokenize the input pair\n",
    "            inputs = tokenizer([premises[i]] * num_hypotheses, hypotheses, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "            # Run the input through the model\n",
    "            logits = model(**inputs.to(DEVICE)).logits\n",
    "\n",
    "            # Get probabilities and labels for the input\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            label_mapping = ['contradiction', 'neutral', 'entailment']\n",
    "            labels = [label_mapping[p.argmax().item()] for p in probabilities]\n",
    "\n",
    "            results[premises[i]] = (labels, convert_probabilities_batched(probabilities, label_mapping))\n",
    "    return results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run data through model, add labels and probabilities"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "source": [
    "def run_bart_model_add_labels(category):\n",
    "    # Create a copy of the input dataframe\n",
    "    df_copy = test_data[category].copy()\n",
    "\n",
    "    # Run BART NLI on the dataframes for the specified category\n",
    "    results = bart_nli_batched_hypotheses(list(df_copy['comment_text']), hypotheses[category])\n",
    "\n",
    "    # Process the results separately and add them to the dataframe\n",
    "    labels_column = []\n",
    "    probabilities_column = []\n",
    "\n",
    "    for premise in df_copy['comment_text']:\n",
    "        if premise in results:\n",
    "            labels, probabilities = results[premise]\n",
    "        else:\n",
    "            labels, probabilities = None, None\n",
    "        labels_column.append(labels)\n",
    "        probabilities_column.append(probabilities)\n",
    "\n",
    "    df_copy['labels'] = labels_column\n",
    "    df_copy['probabilities'] = probabilities_column\n",
    "\n",
    "    return df_copy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "source": [
    "def determine_label_accuracy(df):\n",
    "    def is_correct_bias(row):\n",
    "        return any(label == 'entailment' for label in row['labels'])\n",
    "\n",
    "    def is_correct_neutral(row):\n",
    "        return all(label != 'entailment' for label in row['labels'])\n",
    "\n",
    "    # Apply the functions and store the results in the new column 'is_label_correct'\n",
    "    df['is_label_correct'] = df.apply(lambda row: is_correct_bias(row) if row['bias'] == 'bias' else is_correct_neutral(row), axis=1)\n",
    "    df['predicted'] = True\n",
    "    return df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_results = run_bart_model_add_labels('political')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Calculate f1 scores"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Function to calculate and print F1 score for a given DataFrame\n",
    "def print_f1_score(df, category_name):\n",
    "    f1 = f1_score(df['is_label_correct'], df['predicted'])\n",
    "    print(f\"F1 score for {category_name}: {f1}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "source": [
    "# Read the CSV files into DataFrames\n",
    "df_political_results = pd.read_csv('./results/final/political_results.csv')\n",
    "df_gender_results = pd.read_csv('./results/final/gender_results.csv')\n",
    "df_lgbtq_results = pd.read_csv('./results/final/lgbtq_results.csv')\n",
    "df_race_results = pd.read_csv('./results/final/race_results.csv')\n",
    "df_religion_results = pd.read_csv('./results/final/religion_results.csv')\n",
    "\n",
    "# Calculate and print F1 score for each DataFrame\n",
    "print_f1_score(df_political_results, 'Political')\n",
    "print_f1_score(df_gender_results, 'Gender')\n",
    "print_f1_score(df_lgbtq_results, 'LGBTQ')\n",
    "print_f1_score(df_race_results, 'Race')\n",
    "print_f1_score(df_religion_results, 'Religion')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score for Political: 0.7760532150776054\n",
      "F1 score for Gender: 0.7016574585635359\n",
      "F1 score for LGBTQ: 0.8682774746687452\n",
      "F1 score for Race: 0.7835820895522388\n",
      "F1 score for Religion: 0.8613981762917933\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "source": [
    "# Combine the DataFrames vertically\n",
    "combined_df = pd.concat([df_political_results, df_gender_results, df_lgbtq_results, df_race_results, df_religion_results], ignore_index=True)\n",
    "\n",
    "combined_results = './results/final/combined_results.csv'\n",
    "combined_df.to_csv(combined_results, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "source": [
    "print_f1_score(combined_df, 'Combined')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "F1 score for Combined: 0.8153684210526316\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.12 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}