{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import sys\n",
    "sys.path.append('src/')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# panda view settings\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.max_rows', None)  # Display all rows\n",
    "pd.set_option('display.max_colwidth', 500)  # Display full text in columns"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "DEVICE = torch.device(\n",
    "    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(DEVICE)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df_train = pd.read_csv(\"./data/toxicbias_train.csv\")\n",
    "df_test = pd.read_csv(\"./data/toxicbias_test.csv\")\n",
    "\n",
    "# split up train data into bias and neutral\n",
    "df_bias = df_train[df_train['bias'] == 'bias']\n",
    "df_neutral = df_train[df_train['bias'] == 'neutral']\n",
    "\n",
    "# add extra neutral cases from test data\n",
    "df_test_neutral = df_test[df_test['bias'] == 'neutral']\n",
    "df_neutral = pd.concat([df_neutral, df_test_neutral], ignore_index=True)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split up dataframe by category"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# store unique categories\n",
    "unique_categories = set()\n",
    "\n",
    "# Iterate through each entry in the 'category' column\n",
    "for categories in df_bias['category'].str.split(','):\n",
    "    for category in categories:\n",
    "        stripped_category = category.strip()\n",
    "        if stripped_category and stripped_category.lower() != 'none':\n",
    "            unique_categories.add(stripped_category)\n",
    "\n",
    "unique_categories_list = sorted(list(unique_categories))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "categorical_dfs = {category: pd.DataFrame(columns=df_bias.columns) for category in unique_categories_list}\n",
    "\n",
    "# Split categories and add rows to the corresponding dataframes in dictionary\n",
    "def split_categories_and_add_rows(row):\n",
    "    categories = row['category'].split(',')\n",
    "    for category in categories:\n",
    "        category = category.strip()  # Remove leading/trailing spaces\n",
    "        if category in categorical_dfs:\n",
    "            categorical_dfs[category] = categorical_dfs[category].append(row, ignore_index=True)\n",
    "\n",
    "df_bias.apply(split_categories_and_add_rows, axis=1)\n",
    "\n",
    "# Print the shape of each category dataframe\n",
    "for category, category_df in categorical_dfs.items():\n",
    "    print(f\"Dataframe for {category}:\")\n",
    "    print(f\"Shape: {category_df.shape}\")\n",
    "    print(\"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dataframe for gender:\n",
      "Shape: (293, 6)\n",
      "\n",
      "\n",
      "Dataframe for lgbtq:\n",
      "Shape: (453, 6)\n",
      "\n",
      "\n",
      "Dataframe for political:\n",
      "Shape: (172, 6)\n",
      "\n",
      "\n",
      "Dataframe for race:\n",
      "Shape: (1768, 6)\n",
      "\n",
      "\n",
      "Dataframe for religion:\n",
      "Shape: (1257, 6)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split training and test data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "training_data = {}\n",
    "test_data = {}\n",
    "\n",
    "# Perform the 80-20 split for each category dataframe\n",
    "for category, df in categorical_dfs.items():\n",
    "    train_df, test_df = train_test_split(df, test_size=0.8, random_state=42)\n",
    "    \n",
    "    training_data[category] = train_df\n",
    "    test_data[category] = test_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "for category, train_df in training_data.items():\n",
    "    print('Training data')\n",
    "    print(f\"Dataframe for {category}:\")\n",
    "    print(f\"Shape: {train_df.shape}\")\n",
    "    print(\"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data\n",
      "Dataframe for gender:\n",
      "Shape: (58, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for lgbtq:\n",
      "Shape: (90, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for political:\n",
      "Shape: (34, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for race:\n",
      "Shape: (353, 6)\n",
      "\n",
      "\n",
      "Training data\n",
      "Dataframe for religion:\n",
      "Shape: (251, 6)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "for category, test_df in test_data.items():\n",
    "    print('Test data')\n",
    "    print(f\"Dataframe for {category}:\")\n",
    "    print(f\"Shape: {test_df.shape}\")\n",
    "    print(\"\\n\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test data\n",
      "Dataframe for gender:\n",
      "Shape: (235, 6)\n",
      "\n",
      "\n",
      "Test data\n",
      "Dataframe for lgbtq:\n",
      "Shape: (363, 6)\n",
      "\n",
      "\n",
      "Test data\n",
      "Dataframe for political:\n",
      "Shape: (138, 6)\n",
      "\n",
      "\n",
      "Test data\n",
      "Dataframe for race:\n",
      "Shape: (1415, 6)\n",
      "\n",
      "\n",
      "Test data\n",
      "Dataframe for religion:\n",
      "Shape: (1006, 6)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Hypotheses"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "training_data['gender'].sample(5)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>identity_attack_x</th>\n",
       "      <th>bias</th>\n",
       "      <th>target</th>\n",
       "      <th>category</th>\n",
       "      <th>rationale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>Women are paid less than men. That is a fact. Your comment makes no sense.</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>bias</td>\n",
       "      <td>women</td>\n",
       "      <td>gender</td>\n",
       "      <td>prejudice against women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>How do we fight agaisnt women who use sexual favours to get things they want from men?</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>bias</td>\n",
       "      <td>women</td>\n",
       "      <td>gender</td>\n",
       "      <td>prejudice against women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Why are most conservatives against actual people?  No heart?     Gays shouldnt have any rights Women are too stupid to control their own bodies.  But love the orange toddler</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>bias</td>\n",
       "      <td>gays,women</td>\n",
       "      <td>lgbtq,gender</td>\n",
       "      <td>prejudice against gays and women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>This is not and never has been a case of white mans abuses of natives for sex but of natives abuses of white man for profit.</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>bias</td>\n",
       "      <td>whites,men</td>\n",
       "      <td>race,gender</td>\n",
       "      <td>prejudice against white men</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>Nothing is true for everybody.  But we need to have rules and common practice is to establish them based on what is commonly understood not the exceptions like effeminate men.</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>bias</td>\n",
       "      <td>men</td>\n",
       "      <td>gender</td>\n",
       "      <td>prejudice against men</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                        comment_text  \\\n",
       "249                                                                                                       Women are paid less than men. That is a fact. Your comment makes no sense.   \n",
       "130                                                                                           How do we fight agaisnt women who use sexual favours to get things they want from men?   \n",
       "99     Why are most conservatives against actual people?  No heart?     Gays shouldnt have any rights Women are too stupid to control their own bodies.  But love the orange toddler   \n",
       "191                                                     This is not and never has been a case of white mans abuses of natives for sex but of natives abuses of white man for profit.   \n",
       "280  Nothing is true for everybody.  But we need to have rules and common practice is to establish them based on what is commonly understood not the exceptions like effeminate men.   \n",
       "\n",
       "     identity_attack_x  bias      target      category  \\\n",
       "249           0.259259  bias       women        gender   \n",
       "130           0.500000  bias       women        gender   \n",
       "99            0.608696  bias  gays,women  lgbtq,gender   \n",
       "191           0.500000  bias  whites,men   race,gender   \n",
       "280           0.500000  bias         men        gender   \n",
       "\n",
       "                            rationale  \n",
       "249           prejudice against women  \n",
       "130           prejudice against women  \n",
       "99   prejudice against gays and women  \n",
       "191       prejudice against white men  \n",
       "280             prejudice against men  "
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "hypotheses = {\n",
    "  'gender' : [\n",
    "    'A person belonging to a gender lacks intelligence.',\n",
    "    'A person belonging to a gender is a criminal.',\n",
    "    'A person belonging to a gender is violent.',\n",
    "    'Violence is encouraged against a gender.',\n",
    "    'A person belonging to a gender is abusive.',\n",
    "    'A person belonging to a gender should be blamed.',\n",
    "    'A person belonging to a gender is toxic.',\n",
    "    'A gender is unable to do some jobs.',\n",
    "    'A gender is unable to do something as well as another gender.',\n",
    "    'A person belonging to a gender is effeminate.',\n",
    "    'A person belonging to a gender is compared to an object.',\n",
    "    'A person belonging to a gender uses or should be used for sex.',\n",
    "    'A person belonging to a gender has an advantage compared to others.',\n",
    "    'A person belonging to a gender is misogynist.',\n",
    "    'A person belonging to a gender is disliked.',\n",
    "  ],\n",
    "  'lgbtq' : [\n",
    "    'Lgbtq persons are believed to have a condition.',\n",
    "    'Lgbtq should not be part of society.',\n",
    "    'Violence is encouraged against lgbtq persons.',\n",
    "    'Lgbtq persons are compared to animals.',\n",
    "    'Lgbtq persons are believed to have an agenda.',\n",
    "    'Lgbtq persons should be excluded.',\n",
    "    'Lgbtq persons should stay home.',\n",
    "    'Lgbtq persons commit crimes.',\n",
    "    'Lgbtq persons get preferred to other persons.',\n",
    "    'Lgbtq persons are seen as not being normal.', \n",
    "    'Lgbtq persons should not be allowed to marry.', \n",
    "    'Lgbtq persons have a disease.',\n",
    "    'Lgbtq persons are seen as being disgusting.',\n",
    "    'Lgbtq persons are not tolerated or desired.',\n",
    "    'Lgbtq persons are treated differently.',\n",
    "\n",
    "  ],\n",
    "  'political' : [ \n",
    "      'A person belonging to a political group is stupid.',\n",
    "      'A person belonging to a political group is crazy.',\n",
    "      'Things are worse due to a political group.',\n",
    "      'Members of a political group are lgbtq.',\n",
    "      'Members of a political group are hypocritical.',\n",
    "      'Members of a political group dislike a gender or lgbtq.',\n",
    "      'Members of a political group commit crimes.',\n",
    "      'Members of a political group are violent.',\n",
    "      'Members of a political group are hateful.',\n",
    "      'Members of a political group are compared to Nazis.',\n",
    "      'Members of a political group are racist.',\n",
    "      'A person belonging to a political group has a disease.',\n",
    "      'Members of a political group are taking control.',\n",
    "      'Members of a political group prefer minorities.',\n",
    "      'Members of a political group hate their country or race.',\n",
    "  ],\n",
    "  'race' : [\n",
    "    'Members of a race commit crimes.',\n",
    "    'Members of a race are violent.',\n",
    "    'Members of a race are compared to animals.',\n",
    "    'The use of violence is encouraged or desired against a race.',\n",
    "    'Members of a race should remain in their respective countries.',\n",
    "    'The integration of a race should be avoided in a country or society.',\n",
    "    'Members of a race are a threat to others.',\n",
    "    'A race is inferior when compared to another race.',\n",
    "    'A race is preferred to another.',\n",
    "    'Members of a race are stupid.',\n",
    "    'There are too many members of a race.',\n",
    "    'Members of a race should be excluded.',\n",
    "    'Members of a race should go back to their homes.',\n",
    "    'Members of a race are refused.',\n",
    "    'Members of a race are racist.',\n",
    "  ],\n",
    "  'religion' : [\n",
    "    'The use of violence is encouraged or desired against a religion.',\n",
    "    'A religion should be discriminated against.',\n",
    "    'Members of a religion should be excluded and marginalized.',\n",
    "    'Members of a religion should remain in their respective countries.',\n",
    "    'Members of a religion are compared to animals.',\n",
    "    'A religion or members of a religion are compared to devils or demons.',\n",
    "    'The integration of a religion should be avoided in a country or society.',\n",
    "    'A religion or members of a religion are stupid.',\n",
    "    'A religion is inferior when compared to another religion.',\n",
    "    'A religion is preferred to another.',\n",
    "    'Religious practices of a particular faith should be restricted or banned in public spaces.',\n",
    "    'Religious attire or symbols should be prohibited.',\n",
    "    'A religion promotes violence or intolerance.',\n",
    "    'Members of a religion are a threat.',\n",
    "    'Members of a religion are not trustworthy or loyal.',\n",
    "  ],\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Add neutral cases"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def add_rows_to_dataframes(dataframes_dict):\n",
    "    # Set the random state for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    df_neutral_copy = df_neutral.copy()\n",
    "    max_neutral_length = len(df_neutral_copy)\n",
    "\n",
    "    for key, df in dataframes_dict.items():\n",
    "        # Calculate the maximum number of rows to add\n",
    "        rows_to_add = len(df)\n",
    "\n",
    "        # check if neutral copy array is too low on rows\n",
    "        if len(df_neutral_copy) < rows_to_add:\n",
    "            # reset array to full size\n",
    "            df_neutral_copy = df_neutral.copy()\n",
    "\n",
    "        # check if we need to drop bias data to match number of neutral cases\n",
    "        if rows_to_add > max_neutral_length:\n",
    "            # Drop excess rows from df to match the length of df_neutral_copy\n",
    "            rows_to_drop = np.random.choice(df.index, size=len(df) - len(df_neutral_copy), replace=False)\n",
    "            df.drop(rows_to_drop, inplace=True)\n",
    "\n",
    "        # Choose random rows from df_neutral_copy without replacement\n",
    "        rows_to_add = df_neutral_copy.sample(n=len(df), replace=False)\n",
    "\n",
    "        df = pd.concat([df, rows_to_add], ignore_index=True)\n",
    "\n",
    "         # drop these rows from df_neutral_copy after concatenation\n",
    "        df_neutral_copy.drop(rows_to_add.index, inplace=True)\n",
    "\n",
    "        # Update the dataframe in the dataframes_dict\n",
    "        dataframes_dict[key] = df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "add_rows_to_dataframes(test_data)\n",
    "add_rows_to_dataframes(training_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "for category in test_data:\n",
    "  print(test_data[category]['bias'].value_counts())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bias       235\n",
      "neutral    235\n",
      "Name: bias, dtype: int64\n",
      "neutral    363\n",
      "bias       363\n",
      "Name: bias, dtype: int64\n",
      "bias       138\n",
      "neutral    138\n",
      "Name: bias, dtype: int64\n",
      "bias       998\n",
      "neutral    998\n",
      "Name: bias, dtype: int64\n",
      "bias       998\n",
      "neutral    998\n",
      "Name: bias, dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "for category in training_data:\n",
    "  print(training_data[category]['bias'].value_counts())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "bias       58\n",
      "neutral    58\n",
      "Name: bias, dtype: int64\n",
      "bias       90\n",
      "neutral    90\n",
      "Name: bias, dtype: int64\n",
      "neutral    34\n",
      "bias       34\n",
      "Name: bias, dtype: int64\n",
      "neutral    353\n",
      "bias       353\n",
      "Name: bias, dtype: int64\n",
      "bias       251\n",
      "neutral    251\n",
      "Name: bias, dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deberta v3 model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def convert_probabilities_batched(probabilities, label_mapping):\n",
    "    probabilities_list = probabilities.tolist()  # Convert the tensor to a list\n",
    "    return [{name: round(pred * 100, 1) for pred, name in zip(preds, label_mapping)} for preds in probabilities_list]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "def deberta_v3_nli_batched_hypotheses(premises, hypotheses):\n",
    "    model_name = 'MoritzLaurer/mDeBERTa-v3-base-mnli-xnli'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model.to(DEVICE) \n",
    "\n",
    "    num_premises = len(premises)\n",
    "    num_hypotheses = len(hypotheses)\n",
    "    results = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_premises):\n",
    "            labels = []\n",
    "\n",
    "            # Tokenize the input pair\n",
    "            inputs = tokenizer([premises[i]] * num_hypotheses, hypotheses, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "            # Run the input through the model\n",
    "            logits = model(**inputs.to(DEVICE)).logits\n",
    "\n",
    "            # Get probabilities and labels for the input\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            label_mapping = [\"entailment\", \"neutral\", \"contradiction\"]\n",
    "            labels = [label_mapping[p.argmax().item()] for p in probabilities]\n",
    "\n",
    "            results[premises[i]] = (labels, convert_probabilities_batched(probabilities, label_mapping))\n",
    "    return results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def run_deberta_model_add_labels(category,data_type='training'):\n",
    "    # Create a copy of the input dataframe\n",
    "    if data_type == 'test':\n",
    "      df_copy = test_data[category].copy()\n",
    "    if data_type == 'training':\n",
    "      df_copy = training_data[category].copy()\n",
    "\n",
    "    # Run BART NLI on the dataframes for the specified category\n",
    "    results = deberta_v3_nli_batched_hypotheses(list(df_copy['comment_text']), hypotheses[category])\n",
    "\n",
    "    # Process the results separately and add them to the dataframe\n",
    "    labels_column = []\n",
    "    probabilities_column = []\n",
    "\n",
    "    for premise in df_copy['comment_text']:\n",
    "        if premise in results:\n",
    "            labels, probabilities = results[premise]\n",
    "        else:\n",
    "            labels, probabilities = None, None\n",
    "        labels_column.append(labels)\n",
    "        probabilities_column.append(probabilities)\n",
    "\n",
    "    df_copy['labels'] = labels_column\n",
    "    df_copy['probabilities'] = probabilities_column\n",
    "\n",
    "    return df_copy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "df_pol = run_deberta_model_add_labels('political', 'training')\n",
    "df_race = run_deberta_model_add_labels('race', 'training')\n",
    "df_gender = run_deberta_model_add_labels('gender', 'training')\n",
    "df_religion = run_deberta_model_add_labels('religion', 'training')\n",
    "df_lgbtq = run_deberta_model_add_labels('lgbtq', 'training')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/convert_slow_tokenizer.py:470: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_pol \u001b[38;5;241m=\u001b[39m \u001b[43mrun_deberta_model_add_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpolitical\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtraining\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [20], line 9\u001b[0m, in \u001b[0;36mrun_deberta_model_add_labels\u001b[0;34m(category, data_type)\u001b[0m\n\u001b[1;32m      6\u001b[0m   df_copy \u001b[38;5;241m=\u001b[39m training_data[category]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Run BART NLI on the dataframes for the specified category\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdeberta_v3_nli_batched_hypotheses\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdf_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcomment_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypotheses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Process the results separately and add them to the dataframe\u001b[39;00m\n\u001b[1;32m     12\u001b[0m labels_column \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn [19], line 19\u001b[0m, in \u001b[0;36mdeberta_v3_nli_batched_hypotheses\u001b[0;34m(premises, hypotheses)\u001b[0m\n\u001b[1;32m     16\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer([premises[i]] \u001b[38;5;241m*\u001b[39m num_hypotheses, hypotheses, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Run the input through the model\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Get probabilities and labels for the input\u001b[39;00m\n\u001b[1;32m     22\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1313\u001b[0m, in \u001b[0;36mDebertaV2ForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1305\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1306\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1308\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1309\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1310\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1311\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1313\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1317\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m encoder_layer \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1325\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(encoder_layer)\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:1083\u001b[0m, in \u001b[0;36mDebertaV2Model.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1073\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(input_shape, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1075\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m   1076\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   1077\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1080\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1081\u001b[0m )\n\u001b[0;32m-> 1083\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1084\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1086\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1089\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m encoded_layers \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mz_steps \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:520\u001b[0m, in \u001b[0;36mDebertaV2Encoder.forward\u001b[0;34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[0m\n\u001b[1;32m    511\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[1;32m    512\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    513\u001b[0m         next_kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    517\u001b[0m         rel_embeddings,\n\u001b[1;32m    518\u001b[0m     )\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 520\u001b[0m     output_states \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    530\u001b[0m     output_states, att_m \u001b[38;5;241m=\u001b[39m output_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:372\u001b[0m, in \u001b[0;36mDebertaV2Layer.forward\u001b[0;34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n\u001b[1;32m    371\u001b[0m     attention_output, att_matrix \u001b[38;5;241m=\u001b[39m attention_output\n\u001b[0;32m--> 372\u001b[0m intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintermediate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    374\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/models/deberta_v2/modeling_deberta_v2.py:324\u001b[0m, in \u001b[0;36mDebertaV2Intermediate.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 324\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate_act_fn(hidden_states)\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dfs = [df_pol, df_race, df_gender, df_religion, df_lgbtq]\n",
    "\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('results/training/debertav3/combined_deberta_trainng_results.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bart model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def bart_nli_batched_hypotheses(premises, hypotheses):\n",
    "    model_name = 'facebook/bart-large-mnli'\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "    model.to(DEVICE) \n",
    "\n",
    "    num_premises = len(premises)\n",
    "    num_hypotheses = len(hypotheses)\n",
    "    results = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_premises):\n",
    "            labels = []\n",
    "\n",
    "            # Tokenize the input pair\n",
    "            inputs = tokenizer([premises[i]] * num_hypotheses, hypotheses, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "            # Run the input through the model\n",
    "            logits = model(**inputs.to(DEVICE)).logits\n",
    "\n",
    "            # Get probabilities and labels for the input\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            label_mapping = ['contradiction', 'neutral', 'entailment']\n",
    "            labels = [label_mapping[p.argmax().item()] for p in probabilities]\n",
    "\n",
    "            results[premises[i]] = (labels, convert_probabilities_batched(probabilities, label_mapping))\n",
    "    return results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def run_bart_model_add_labels(category,data_type='training'):\n",
    "    # Create a copy of the input dataframe\n",
    "    if data_type == 'test':\n",
    "      df_copy = test_data[category].copy()\n",
    "    if data_type == 'training':\n",
    "      df_copy = training_data[category].copy()\n",
    "\n",
    "    # Run BART NLI on the dataframes for the specified category\n",
    "    results = bart_nli_batched_hypotheses(list(df_copy['comment_text']), hypotheses[category])\n",
    "\n",
    "    # Process the results separately and add them to the dataframe\n",
    "    labels_column = []\n",
    "    probabilities_column = []\n",
    "\n",
    "    for premise in df_copy['comment_text']:\n",
    "        if premise in results:\n",
    "            labels, probabilities = results[premise]\n",
    "        else:\n",
    "            labels, probabilities = None, None\n",
    "        labels_column.append(labels)\n",
    "        probabilities_column.append(probabilities)\n",
    "\n",
    "    df_copy['labels'] = labels_column\n",
    "    df_copy['probabilities'] = probabilities_column\n",
    "\n",
    "    return df_copy"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pol = run_bart_model_add_labels('political', 'training')\n",
    "df_race = run_bart_model_add_labels('race', 'training')\n",
    "df_gender = run_bart_model_add_labels('gender', 'training')\n",
    "df_religion = run_bart_model_add_labels('religion', 'training')\n",
    "df_lgbtq = run_bart_model_add_labels('lgbtq', 'training')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pol.to_csv('results/training/bart-large/political_training_results.csv')\n",
    "df_race.to_csv('results/training/bart-large/race_training_results.csv')\n",
    "df_gender.to_csv('results/training/bart-large/gender_training_results.csv')\n",
    "df_religion.to_csv('results/training/bart-large/religion_training_results.csv')\n",
    "df_lgbtq.to_csv('results/training/bart-large/lgbtq_training_results.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get test results"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pol = run_deberta_model_add_labels('political', 'test')\n",
    "df_race = run_deberta_model_add_labels('race', 'test')\n",
    "df_gender = run_deberta_model_add_labels('gender', 'test')\n",
    "df_religion = run_deberta_model_add_labels('religion', 'test')\n",
    "df_lgbtq = run_deberta_model_add_labels('lgbtq', 'test')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pol.to_csv('results/test/debertav3/political_labels_deberta_test.csv')\n",
    "df_race.to_csv('results/test/debertav3/race_labels_deberta_test.csv')\n",
    "df_gender.to_csv('results/test/debertav3/gender_labels_deberta_test.csv')\n",
    "df_religion.to_csv('results/test/debertav3/religion_labels_deberta_test.csv')\n",
    "df_lgbtq.to_csv('results/test/debertav3/lgbtq_labels_deberta_test.csv')\n",
    "\n",
    "dfs = [df_pol, df_race, df_gender, df_religion, df_lgbtq]\n",
    "\n",
    "# Concatenate the dataframes vertically\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('results/test/debertav3/combined_test_results_deberta.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pol = run_bart_model_add_labels('political', 'test')\n",
    "df_race = run_bart_model_add_labels('race', 'test')\n",
    "df_gender = run_bart_model_add_labels('gender', 'test')\n",
    "df_religion = run_bart_model_add_labels('religion', 'test')\n",
    "df_lgbtq = run_bart_model_add_labels('lgbtq', 'test')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_pol.to_csv('results/test/bart-large/political_test_results.csv')\n",
    "df_race.to_csv('results/test/bart-large/race_test_results.csv')\n",
    "df_gender.to_csv('results/test/bart-large/gender_test_results.csv')\n",
    "df_religion.to_csv('results/test/bart-large/religion_test_results.csv')\n",
    "df_lgbtq.to_csv('results/test/bart-large/lgbtq_test_results.csv')\n",
    "\n",
    "dfs = [df_pol, df_race, df_gender, df_religion, df_lgbtq]\n",
    "\n",
    "# Concatenate the dataframes vertically\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "combined_df.to_csv('results/test/bart-large/combined_test_results.csv')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fine tuning"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import random\n",
    "\n",
    "sentences = [\n",
    "    \"Members of a race commit crimes.\",\n",
    "    \"Members of a race are violent.\",\n",
    "    \"Members of a race are compared to animals.\",\n",
    "    \"The use of violence is encouraged or desired against a race.\",\n",
    "    \"Members of a race should remain in their respective countries.\",\n",
    "    \"The integration of a race should be avoided in a country or society.\",\n",
    "    \"Members of a race are a threat to others.\",\n",
    "    \"A race is inferior when compared to another race.\",\n",
    "    \"A race is preferred to another.\",\n",
    "    \"Members of a race are stupid.\",\n",
    "    \"There are too many members of a race.\",\n",
    "    \"Members of a race should be excluded.\",\n",
    "    \"Members of a race should go back to their homes.\",\n",
    "    \"Members of a race are refused.\",\n",
    "    \"Members of a race are racist.\"\n",
    "]\n",
    "\n",
    "num_sentences_to_print = 1300\n",
    "\n",
    "for _ in range(num_sentences_to_print):\n",
    "    random_sentence = random.choice(hypotheses['religion'])\n",
    "    print(random_sentence)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "input_file = \"data/fine_tuning_balanced_religion.csv\"\n",
    "\n",
    "fine_tuning = pd.read_csv(input_file)\n",
    "\n",
    "# Drop rows where the 'hypothesis' column is empty\n",
    "fine_tuning = fine_tuning.dropna(subset=['hypothesis'])\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Check for null values in the 'Label' column\n",
    "null_rows = fine_tuning[fine_tuning['label'].isnull() | fine_tuning['hypothesis'].isnull() | fine_tuning['premise'].isnull()]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "model_name = 'facebook/bart-large-mnli'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "## split data into training and validation \n",
    "train_dataset, val_dataset = train_test_split(fine_tuning, test_size=0.1, random_state=42)\n",
    "\n",
    "train_dataset[\"label\"] = [int(label) for label in train_dataset[\"label\"]]\n",
    "val_dataset[\"label\"] = [int(label) for label in val_dataset[\"label\"]]\n",
    "\n",
    "# Create Hugging Face Dataset objects\n",
    "train_data = Dataset.from_dict(train_dataset)\n",
    "\n",
    "eval_data = Dataset.from_dict(val_dataset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"hypothesis\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_data = train_data.map(tokenize_function, batched=True)\n",
    "eval_data = eval_data.map(tokenize_function, batched=True)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aecc0de071954a88ab3244af5de5ea3f"
      },
      "text/plain": [
       "Map:   0%|          | 0/943 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5a6019d9de9946ee9997c4b8626e9c8e"
      },
      "text/plain": [
       "Map:   0%|          | 0/105 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {}
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Set up Trainer and TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=1,  # Decrease batch size\n",
    "    fp16=True,  # Enable mixed precision training\n",
    "    max_grad_norm=1.0,  # Implement gradient clipping\n",
    "    eval_steps=100,  # Limit evaluation steps\n",
    "    save_total_limit=1,  # Save only the best checkpoint to save memory\n",
    ")\n",
    "\n",
    "# Metric definition and computation\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    inner_logits = logits[0]  # Extract the inner logits array\n",
    "    predictions = np.argmax(inner_logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Initialize Trainer with updated args\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=eval_data,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "model.save_pretrained(\"fine-tuning/bart_fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"fine-tuning/bart_fine_tuned_tokenizer\")"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine-tuning/bart_fine_tuned_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine-tuning/bart_fine_tuned_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run fine tuned mode"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def bart_nli_batched_hypotheses(premises, hypotheses):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('fine-tuning/bart_fine_tuned_model')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('fine-tuning/bart_fine_tuned_tokenizer', use_fast=True)\n",
    "    model.to(DEVICE) \n",
    "\n",
    "    num_premises = len(premises)\n",
    "    num_hypotheses = len(hypotheses)\n",
    "    results = {}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_premises):\n",
    "            labels = []\n",
    "\n",
    "            # Tokenize the input pair\n",
    "            inputs = tokenizer([premises[i]] * num_hypotheses, hypotheses, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "            # Run the input through the model\n",
    "            logits = model(**inputs.to(DEVICE)).logits\n",
    "\n",
    "            # Get probabilities and labels for the input\n",
    "            probabilities = torch.softmax(logits, dim=1)\n",
    "            label_mapping = ['contradiction', 'neutral', 'entailment']\n",
    "            labels = [label_mapping[p.argmax().item()] for p in probabilities]\n",
    "\n",
    "            results[premises[i]] = (labels, convert_probabilities_batched(probabilities, label_mapping))\n",
    "    return results\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_religion = run_bart_model_add_labels('religion', 'test')\n",
    "\n",
    "df_religion.to_csv('results/fine_tuning/bart-large/fine_tuning_religion_test_results.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.12 64-bit"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}